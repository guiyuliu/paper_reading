

# PV

FCOS3D

PGD

MonoATT

### MonoDTR

- #### DFE module 

  - 训练的时候有depth监督，作为辅助3D检测的模块，相比于以前复杂的深度网络结构，DFE 模块轻量化

- #### Depth-aware Transformer

  - 

- #### 2d-3d detection head

  - 2D 回归:  x,y, w, h 

  - 3D 回归:  3D 投影到2D上的点，以及z(depth)，观测角

  - single-stage detector with predifined  2D-3D anchors (two stage, proposal（只分前背景/） rcnn 做roi align之后再回归)

  - 有anchor，每个位置直接预测anchor对应的框

    - follow yolo v3， 用三种尺寸的特征图来做预测，为小尺寸的特征图分配大尺寸的锚点框，适合检测大目标，为大尺寸的特征图分配小尺寸的锚点框，适合检测小目标
    - 每种尺寸的特征图每个grid上面有3种类型的锚点框，输出 $$ N\*N\*[3*(4+1+80)] $$ ,N 为feature map的尺寸，3 表示3种锚点框，4表示4个坐标值，1表示是否包含物体的置信度，80表示coco数据集的类别总数

  - NMS

  - reg loss 是既有2D 又有3D ？

  - 训练阶段：

    - 将gt投影到2D空间，跟2D anchors做IOU, IOU >0.5的匹配上对应的3D box做优化

  - get_bboxes

    - 预测出来的2D 框做NMS
    - post_optimization


#### MonoDETR

DETR based detection head， no NMS



# BEV

### BEVFormer

#### temporal self-attention

当前帧和历史帧的bev queries做self-attention

bev queries 具体是什么表征



#### spatial cross-attention

由bev queries 生成的reference point，bev grid点转换到真实ego坐标系下的点，we predefine a set of anchor heights,



对于cross attention，多视角的图像计算量太大，采用deformable attention，

 BEV query Qp only interacts with its regions of interest across camera views. 

lift each query on the BEV plane to a pillarlike query，and then project these points to 2D views.

**将bev 上的query lift到空间中的pillar中**，投影到2D view上，只在hit view上提取reference point周围的feature，采样到的feature取权重和作为spatial cross-attention的输出



去掉deformable，会怎么样？？如何替代deformable，使之更适合于硬件？





## BEVDet

- 确定了一个bev下纯视觉的范式,统一了bev的检测和分割：image encoder， view transform-(pv2bev)，taskspecific head
- customized data augmentation strategy
- scaled NMS

纯视觉的bevdet范式调整到和fcos3d 与pgd差不多的参数量，会存在overfitting的问题，主要是因为

1.训练数据量不足， 另batch size 得是n=6的倍数

2.augmentation 在pv2bev的过程中会损失掉部分信息，lss的pv2bev是pixel-wise的transform，作用在pv上的image augmentation在到bev的过程中会有一定的损失

### data augmentation

- IDA- Image-view-space augmentation 加在图像上的augmentation matrix 在进行view transform之前会被消除，因此加在pv上的augmentation不会对bev space产生任何影响

- BDA - BEV-space data augmentation. Flipping, scaling, rotation, the operations are conducted both on the output fea ture of the view transformer and the 3D object detection targets to keep their spatial consistency.  这些augmentation会同时加在pv的输出和3d 目标检测的目标上以保持空间的一致性 - -怎么加在目标检测上？

  - 该方法的前提：that the view transformer can decouple the image- view encoder from the subsequent module. pv2bev这个模块能解耦前面的encoder和后面的模块，在其他范式中可能并不适用

###  scale NMS

###  ablation study 

-  有bevencoder，只加IDA map并没有什么大的提升不足1%，加了BDA效果提升明显约 6.2%，两者都加提升更大13%
-  无bevencoder， IDA提升 5.4%，BDA 提升0.9%， 两者都加提升8.4%

*BEVDET4D*

## Multi-view PV

## *PETR*

multi-view ， transformer based 检测head， 对DETR3D的一种改进。

为了解决什么问题：

- DETR3D中 预测出来的reference point的坐标并不准，因此投影到2d feature上采样，可能采不到东西。其次只有采样点的feature，才会被收集，因此无法进行全局的表征学习。

- 将3d position 编码进2d feature。还是pv feature，没有lift和splat过程，生成3d world spcae 中的位置坐标，将这些位置信息编码进去。 query 还是随机生成


3D coordinates 到3D Positional embedding是如何编码的

## PETRV2

BEVFormer/BEVDet 4D 在bev 空间中对其多frame的特征 

相对于PETR，引入时序

t frame的feature 和 t-1 frame的feature 拼接 ，3D coordinates，经过alignment之后也拼接， 一起送入 Feaure guided Position encoder

FPE： feature position encoder，输出的值作为transformer decoder的key 和 value ，因为3D 编码还要引入2D图像特征，深度值

如何克服object的运动问题？

![image-20230530112515789](assets/视觉3d检测/image-20230530112515789.png)

## StreamPETR

时序的好处：1 遮挡，2 速度的输出

多视：将多个视pv视角转换到一帧统一的3D空间中

- BEV-based methods

- sparse query based methods


**cross-attention:** 是一种feature aggregation的方式，image tokens 和3D object queries之间的关系，global attention PETR，是稠密的，sparse objective attention DETR 3D，是稀疏的，

稀疏和稠密相当于cross attention中 key 和 value 是整张的image feature， 还是只包含物体的位置的feature 

#### temporal modeling

- bev 时序建模 :1. align the historical k BEV features 然后与当前帧拼接 2. 通过 BEV的hidden states 以迭代的方式传播， 但是bev 时序融合仅考虑静态的BEV features 忽视了物体的运动，造成了空间上的错位

- PV 时序建模： 通过object queries 和 perspective feature 做cross attention,  增加时序的建模会增加computation cost

- object-centric 时序建模：只建模temporal之间的交互， 用一个隐式的线性速度模型来编码motion相关的状态，再用global attention 传播时序信息

- bevformer 首次引入时序的modeling ，temporal self-attention 

  bevdet系列， 用拼接feature

  PETRV2:  **global cross attention** ？

  DETR4D/Sparse4D, spare attention to model 多帧间的交互

#### memory queue

- 维护一个N x K的object 队列，N=4， k =256  
- N 不是4吗？为啥是8 frames呢？

#### MLN motion aware normalization

- 

#### hybrid attention

- decoder的self attention中的v 和 k 添加了之前帧的object 信息

#### 其他

-   transformer decoder 只有六层，没有feature 拼接，只有object的memory queue

  









