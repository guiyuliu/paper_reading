# PV

FCOS3D

DETR3D

PGD

# BEV
## *BEVFormer*
## *BEVDET4D*
## BEVDet

- 确定了一个bev下纯视觉的范式,统一了bev的检测和分割：image encoder， view transform-(pv2bev)，taskspecific head
- customized data augmentation strategy
- scaled NMS

纯视觉的bevdet范式调整到和fcos3d 与pgd差不多的参数量，会存在overfitting的问题，主要是因为

1.训练数据量不足， 另batch size 得是n=6的倍数

2.augmentation 在pv2bev的过程中会损失掉部分信息，lss的pv2bev是pixel-wise的transform，作用在pv上的image augmentation在到bev的过程中会有一定的损失

### data augmentation

- IDA- Image-view-space augmentation 加在图像上的augmentation matrix 在进行view transform之前会被消除，因此加在pv上的augmentation不会对bev space产生任何影响
$ p_{image}^{'} = A*P_{image} $ 

- BDA - BEV-space data augmentation. Flipping, scaling, rotation, the operations are conducted both on the output fea ture of the view transformer and the 3D object detection targets to keep their spatial consistency.  这些augmentation会同时加在pv的输出和3d 目标检测的目标上以保持空间的一致性 - -怎么加在目标检测上？

  - 该方法的前提：that the view transformer can decouple the image- view encoder from the subsequent module. pv2bev这个模块能解耦前面的encoder和后面的模块，在其他范式中可能并不适用

###  scale NMS

###  ablation study 

-  有bevencoder，只加IDA map并没有什么大的提升不足1%，加了BDA效果提升明显约 6.2%，两者都加提升更大13%
- 无bevencoder， IDA提升 5.4%，BDA 提升0.9%， 两者都加提升8.4%

## *PETR*

multi-view ， transformer based 检测head， 对DETR3D的一种改进。

为了解决什么问题：

- DETR3D中 预测出来的reference point的坐标并不准，因此投影到2d feature上采样，可能采不到东西。其次只有采样点的feature，才会被收集，因此无法进行全局的表征学习。

- 将3d position 编码进2d feature。还是pv feature，没有lift和splat过程，生成3d world spcae 中的位置坐标，将这些位置信息编码进去。 query 还是随机生成









